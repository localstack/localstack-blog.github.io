<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=stylesheet href=/css/vendor/custom.css><link rel=preload as=font href=/fonts/vendor/jost/jost-v4-latin-regular.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=/fonts/vendor/jost/jost-v4-latin-700.woff2 type=font/woff2 crossorigin><link rel=stylesheet href=/main.b93f21975a8c509862bc461126163a533d1083c67ad9fe8621b9b49c72f22af2f868498b9be3305e5c0243d5b194d4e9df7ec3a49e44fa797de7700c870708a0.css integrity="sha512-uT8hl1qMUJhivEYRJhY6Uz0Qg8Z62f6GIbm0nHLyKvL4aEmLm+MwXlwCQ9WxlNTp337DpJ5E+nl953AMhwcIoA==" crossorigin=anonymous><noscript><style>img.lazyload{display:none}</style></noscript><meta name=robots content="index, follow"><meta name=googlebot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=bingbot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><title>Developing cloud AI-powered apps with LocalStack and Ollama - LocalStack</title><meta name=description content="Integrating Ollama and LocalStack offers a powerful solution for developing and testing cloud AI applications cost-effectively. Ollama simplifies interactions with large language models, while LocalStack emulates AWS services locally, allowing developers to thoroughly test and validate AI functionalities in a controlled environment."><link rel=canonical href=/2024-07-04-develop-cloud-ai-with-ollama-localstack/><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="https://blog.localstack.cloud/2024-07-04-develop-cloud-ai-with-ollama-localstack/localstack-ollama-banner.png"><meta name=twitter:title content="Developing cloud AI-powered apps with LocalStack and Ollama"><meta name=twitter:description content="Integrating Ollama and LocalStack offers a powerful solution for developing and testing cloud AI applications cost-effectively. Ollama simplifies interactions with large language models, while LocalStack emulates AWS services locally, allowing developers to thoroughly test and validate AI functionalities in a controlled environment."><meta name=twitter:site content="@localstack"><meta name=twitter:creator content="@localstack"><meta property="og:title" content="Developing cloud AI-powered apps with LocalStack and Ollama"><meta property="og:description" content="Integrating Ollama and LocalStack offers a powerful solution for developing and testing cloud AI applications cost-effectively. Ollama simplifies interactions with large language models, while LocalStack emulates AWS services locally, allowing developers to thoroughly test and validate AI functionalities in a controlled environment."><meta property="og:type" content="article"><meta property="og:url" content="/2024-07-04-develop-cloud-ai-with-ollama-localstack/"><meta property="og:image" content="/2024-07-04-develop-cloud-ai-with-ollama-localstack/localstack-ollama-banner.png"><meta property="article:published_time" content="2024-07-08T00:00:00+00:00"><meta property="article:modified_time" content="2024-07-08T00:00:00+00:00"><meta property="og:site_name" content="LocalStack"><meta property="article:publisher" content="https://www.facebook.com/"><meta property="article:author" content="https://www.facebook.com/"><meta property="og:locale" content="en_US"><script type=application/ld+json>{"@context":"http://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Home","item":"https:\/\/blog.localstack.cloud\/"},{"@type":"ListItem","position":3,"name":"2024 07 04 Develop Cloud Ai With Ollama Localstack","item":"https:\/\/blog.localstack.cloud\/\/2024-07-04-develop-cloud-ai-with-ollama-localstack\/"}]}</script><meta name=theme-color content="#fff"><link rel=apple-touch-icon sizes=180x180 href=/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=/favicon-16x16.png><link rel=mask-icon href=/safari-pinned-tab.svg color=#6b7cf5><link rel=manifest href=/site.webmanifest><script>(function(a,e,f,g,b,c,d){a.GoogleAnalyticsObject=b,a[b]=a[b]||function(){(a[b].q=a[b].q||[]).push(arguments)},a[b].l=1*new Date,c=e.createElement(f),d=e.getElementsByTagName(f)[0],c.async=1,c.src=g,d.parentNode.insertBefore(c,d)})(window,document,'script','https://www.google-analytics.com/analytics.js','ga'),ga('create','UA-101988473-1','auto'),ga('send','pageview')</script><script async src="https://www.googletagmanager.com/gtag/js?id=G-QRFTZPWGB7"></script><script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-QRFTZPWGB7')</script><script defer src=https://unpkg.com/@tinybirdco/flock.js data-host=https://api.tinybird.co data-token=p.eyJ1IjogIjAwZDRjYzRjLTg4N2UtNGNmZS05YzY4LTJjNzMyNjE5ODdjMCIsICJpZCI6ICI2YzRhMDViNi1iZjVlLTRkY2QtODNlZS1jMmFjYjg5ZjQ1NzAifQ.EPdBzyAb28wUbtZacobblsMi42IwTd7d2amppDfw-jE></script><script type=text/javascript id=hs-script-loader async defer src=//js-eu1.hs-scripts.com/26596507.js></script><script type=text/javascript src=//js-eu1.hsforms.net/forms/embed/v2.js></script><script>function toggleTag(b){const c='tag-'+b;if(c!=='tag-all'){const a=document.getElementsByClassName('tag-all');for(let b=0;b<a.length;b++){const c=a[b];c.classList.add('d-none')}}const d=document.getElementsByClassName(c);for(let a=0;a<d.length;a++){const b=d[a];b.classList.remove('d-none')}const e=document.getElementsByClassName('filterButton');for(let a=0;a<e.length;a++){const b=e[a];b.classList.remove('bg-purple'),b.classList.remove('btn-primary'),b.classList.add('btn-outline-primary')}const f=b+"FilterButton",a=document.getElementById(f);a.classList.add('bg-purple'),a.classList.add('btn-primary'),a.classList.remove('btn-outline-primary')}</script></head><body class="page single"><header></header><section class="section section-sm bg-gradient-dark"><div class="position-absolute w-100 section pb-7 pt-7 d-none d-md-block"><div class=container><div class="row justify-content-center align-items-center"><div class=col-2><img class=w-100 style=transform:scale(2);opacity:.65 src=/images/heroes/blog.svg></div><div class=col-2></div><div class=col-2></div><div class=col-2><img class=w-100 style=transform:rotate(35deg)scale(1.4);opacity:.9 src=/images/heroes/blog.svg></div><div class=col-2><img class=w-100 style=transform:rotate(-45deg)scaleX(-1)scale(1.5);opacity:.75 src=/images/heroes/blog.svg></div></div></div></div><div class="container pb-5 pt-6 pb-md-7 pt-sm-7"><div class="row justify-content-center align-items-center text-white"><h1 class="text-center m-0 pt-lg-5">Developing cloud AI-powered apps with LocalStack and Ollama</h1></div></div></section><section class="section section-sm mb-6"><div class="container px-lg-8"><div class="row justify-content-center"><div class=col-12><article><div class=blog-header><div class="d-flex align-items-center flex-wrap card-footer-text"><p class=mb-0><img src=/images/common/clock.svg>
11&nbsp;MIN READ&nbsp;&#8212;&nbsp;
posted July 8, 2024 by <a class="stretched-link position-relative">Anca Ghenade</a></p><p></div></div><p class=lead>Integrating Ollama and LocalStack offers a powerful solution for developing and testing cloud AI applications cost-effectively. Ollama simplifies interactions with large language models, while LocalStack emulates AWS services locally, allowing developers to thoroughly test and validate AI functionalities in a controlled environment.</p><img class="img-simple img-fluid lazyload blur-up" src=/2024-07-04-develop-cloud-ai-with-ollama-localstack/localstack-ollama-banner_hub8d8eaf3e690c4eda62cd9a0b763b6b4_498464_20x0_resize_box_2.png data-src=/2024-07-04-develop-cloud-ai-with-ollama-localstack/localstack-ollama-banner.png width=1920 height=1080 alt="LocalStack and Ollama"><h2 id=introduction>Introduction</h2><p>In todayâ€™s tech landscape, large language models (LLMs) and AI are transforming both tech-centric and traditional businesses.
AI functionality is being integrated across various platforms to enhance user experience. However, developing and testing these AI
integrations often requires extensive infrastructure, leading to high costs. LocalStack enables developers to build and test
AI integrations locally, which accelerates the development process and avoids extra expenses.</p><p>In this tutorial, we&rsquo;ll explore building an AI-powered chatbot using Ollama, a tool that lets users interact with information
through natural language. For instance, on a government website, rather than navigating complex menus, you could ask a chatbot to find
a specific form. We&rsquo;ll start by setting up the entire system locally with LocalStack, then move to deploying it in the cloud.</p><h2 id=prerequisites>Prerequisites</h2><ul><li><a href=https://aws.amazon.com/free/>AWS free tier account</a></li><li><a href=https://app.localstack.cloud/sign-up>LocalStack Pro</a></li><li><a href=https://docs.docker.com/get-docker/>Docker</a> - for running LocalStack</li><li><a href=https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html>AWS CLI</a> and <a href=https://github.com/localstack/awscli-local>AWS CLI local</a></li><li><a href=https://docs.npmjs.com/downloading-and-installing-node-js-and-npm>npm</a> - for building the frontend app</li></ul><h2 id=architecture-overview>Architecture Overview</h2><p>To follow along with this post and get the React app, you can clone <a href=https://github.com/localstack-samples/sample-ollama-ecs-fargate-alb><strong>the repository</strong></a> for this project.</p><p>We will explore a comprehensive example of running Ollama on ECS Fargate. This example includes a backend with a VPC, a load balancer,
multiple security groups, and an ECR service hosting our image. For simplicity, the frontend application will be a basic chat interface
with a prompt field and an answer box.</p><img class="img-simple img-fluid lazyload blur-up" src=/2024-07-04-develop-cloud-ai-with-ollama-localstack/chatbot-diagram_hu01640bf4e53167856424ab4c1c5c88a8_528427_20x0_resize_box_2.png data-src=/2024-07-04-develop-cloud-ai-with-ollama-localstack/chatbot-diagram.png width=1611 height=712 alt="Stack Diagram"><p>The React application could make a direct request to our Ollama container, but in the real world it&rsquo;s not likely that you would run
just one task and certainly not trying to access the container using its IP address. Ideally you should have multiple tasks running to ensure
high availability, in case one of them encounters an issue, you can always rely on the others to take over. In front of the tasks you&rsquo;ll need
an application load balancer to handle the HTTP requests. This is how traffic is distributed across the containers. The load balancer will
have a listener, which listens for client requests. The requests are routed to targets, which will be the IPs for the tasks/containers. The
targets live in a target group, and that allows us to make configurations for all of them (for example, setting a routing algorithm, or healthcheck
related configs).
Our load balancer needs a security group that allows inbound traffic, and a second security group that only allows incoming traffic
to the ECS service, which will run two tasks.</p><h2 id=why-ollama--tinyllama>Why Ollama & Tinyllama</h2><img class="img-simple img-fluid lazyload blur-up" src=/2024-07-04-develop-cloud-ai-with-ollama-localstack/tinyllama_huc153ff5434aa68ccf08cd0d4cea43e57_1895133_20x0_resize_box_2.png data-src=/2024-07-04-develop-cloud-ai-with-ollama-localstack/tinyllama.png width=1540 height=1290 alt=Tinyllama><p><a href=https://ollama.com/><strong>Ollama</strong></a> is an open-source platform that allows users to run large language models (LLMs) locally on their devices.
In its essence, Ollama streamlines the tasks of downloading, installing, and utilizing a broad spectrum of LLMs, enabling users to discover
their potential without requiring deep technical skills or dependence on cloud-based platforms. Most importantly, Ollama
allows users to run their own specialized LLMs with ease.</p><p>Both tools are designed for local development, so using Ollama with LocalStack for building cloud applications offers several advantages:</p><ul><li><p><strong>Complete local development environment</strong>: Combining Ollama and LocalStack allows developers to run complex cloud applications
entirely on their local machines. Ollama can handle large language models locally, while LocalStack handles the AWS services,
creating a comprehensive and integrated local development environment.</p></li><li><p><strong>Cost efficiency</strong>: Running applications locally avoids the costs associated with cloud resources during development and testing.
This is particularly useful when working with large language models that can be resource-intensive and expensive to run in the cloud.</p></li><li><p><strong>Faster iteration cycles</strong>: Local development with Ollama and LocalStack allows for rapid prototyping and testing. Developers can
quickly make changes and see results without the delay of deploying to the cloud. This speeds up the development cycle significantly.</p></li><li><p><strong>Consistent development and production environments</strong>: By using LocalStack to emulate AWS services, developers can ensure that their
local development environment closely matches the production environment. This reduces the risk of environment-specific bugs and
improves the reliability of the application when deployed to the actual cloud.</p></li><li><p><strong>Improved Testing Capabilities</strong>: LocalStack provides a robust platform for testing AWS services, including ECS and Fargate.
Running Ollama as a Fargate task on LocalStack allows for testing complex deployment scenarios and interactions with other AWS
services, ensuring that the application behaves as expected before deploying to the cloud.</p></li></ul><p><a href=https://arxiv.org/pdf/2401.02385><strong>Tinyllama</strong></a> is a compact AI language model that stands out due to its efficient size and robust training. It occupies just 637 MB
and was trained on a trillion tokens, making it not only mobile-friendly but also powerful enough to surpass similar-sized
models. Designed as a smaller version of Meta&rsquo;s Llama 2, it shares the same architecture and tokenizer,
making it an ideal choice for development and testing, particularly with applications demanding a restricted
computation and memory footprint. Depending on your needs, you can also replace it with a different, or more specialized model.</p><h2 id=running-the-application-on-localstack>Running the application on LocalStack</h2><h3 id=starting-localstack>Starting LocalStack</h3><p>First thing we need to do is start LocalStack by using docker compose. This permits an easy visualisation of all the necessary configs.
Remember to set your <code>LOCALSTACK_AUTH_TOKEN</code> as an environment variable.</p><pre><code class=language-bash>cd localstack
export LOCALSTACK_AUTH_TOKEN=&lt;your_auth_token&gt;
docker compose up
</code></pre><h5 id=some-important-configs>Some important configs</h5><p>All the following configuration flags can be found in the <code>docker-compose.yml</code> file, in the <code>localstack</code> folder:</p><ul><li><strong>DEBUG=1</strong> - This flag enables debug mode in LocalStack. When set to 1, LocalStack provides more detailed logs, making it easier to trace issues.</li><li><strong>ENFORCE_IAM=1</strong> - This configuration enables the enforcement of AWS IAM policies in LocalStack. Normally, LocalStack runs with simplified or no security checks to facilitate development.</li><li><strong>ECS_DOCKER_FLAGS=-e OLLAMA_ORIGINS="*"</strong> - This setting is used to pass environment variables to Docker containers spawned by the ECS service within LocalStack.
Specifically, we set OLLAMA_ORIGINS="*" inside these containers to indicate that requests from any origin are allowed. This is relevant when integrating with web applications that may call APIs from various domains.</li><li><strong>DISABLE_CORS_CHECKS=1</strong> - This flag disables CORS checks in LocalStack, for ease of development.</li><li><strong>DISABLE_CUSTOM_CORS_S3=1</strong> - When set, this configuration disables the custom CORS handling for S3 services within LocalStack.</li></ul><h3 id=building-the-react-app>Building the React app</h3><p>In the <code>localstack</code> folder, thereâ€™s a directory called <code>frontend</code>. To build the React application run
the following commands:</p><pre><code class=language-bash>cd frontend/chatbot/
npm install
npm run build
</code></pre><p>Notice the creation of the <code>build</code> folder. The <code>npm run build</code> command will create the static assets needed to run our
app, and they will then be uploaded to the S3 bucket.</p><h3 id=creating-the-stack>Creating the stack</h3><p>Now we can run the bash script containing AWS CLI commands to create the necessary resources. Letâ€™s first have a look at some of the
commands in the script and identify the resources they create:</p><pre><code class=language-bash>export VPC_ID=$(awslocal ec2 create-vpc --cidr-block 10.0.0.0/16 | jq -r '.Vpc.VpcId')
</code></pre><p>Creates a Virtual Private Cloud (VPC) with a specified CIDR block.</p><pre><code class=language-bash>export SUBNET_ID1=$(awslocal ec2 create-subnet \
  --vpc-id $VPC_ID \
  --cidr-block 10.0.1.0/24 \
  --availability-zone us-east-1a \
  | jq -r '.Subnet.SubnetId')
  
export SUBNET_ID2=$(awslocal ec2 create-subnet \
  --vpc-id $VPC_ID \
  --cidr-block 10.0.2.0/24 \
  --availability-zone us-east-1b \
  | jq -r '.Subnet.SubnetId')
</code></pre><p>Creates two subnets within the VPC, each in a different availability zone.</p><pre><code class=language-bash>export INTERNET_GW_ID=$(awslocal ec2 create-internet-gateway | jq -r '.InternetGateway.InternetGatewayId')

awslocal ec2 attach-internet-gateway \
  --internet-gateway-id $INTERNET_GW_ID \
  --vpc-id $VPC_ID
</code></pre><p>Creates an internet gateway and attaches it to the VPC.</p><pre><code class=language-bash>export RT_ID=$(awslocal ec2 create-route-table --vpc-id $VPC_ID | jq -r '.RouteTable.RouteTableId')

awslocal ec2 associate-route-table \
  --route-table-id $RT_ID \
  --subnet-id $SUBNET_ID1
  
awslocal ec2 associate-route-table \
  --route-table-id $RT_ID \
  --subnet-id $SUBNET_ID2
  
awslocal ec2 create-route \
  --route-table-id $RT_ID \
  --destination-cidr-block 0.0.0.0/0 \
  --gateway-id $INTERNET_GW_ID
</code></pre><p>Creates a route table, associates it with the subnets, and adds a route to the internet gateway.</p><pre><code class=language-bash>export SG_ID1=$(awslocal ec2 create-security-group \
  --group-name ApplicationLoadBalancerSG \
  --description &quot;Security Group of the Load Balancer&quot; \
  --vpc-id $VPC_ID | jq -r '.GroupId')
  
awslocal ec2 authorize-security-group-ingress \
  --group-id $SG_ID1 \
  --protocol tcp \
  --port 80 \
  --cidr 0.0.0.0/0

export SG_ID2=$(awslocal ec2 create-security-group \
  --group-name ContainerFromLoadBalancerSG \
  --description &quot;Inbound traffic from the First Load Balancer&quot; \
  --vpc-id $VPC_ID \
  | jq -r '.GroupId')
  
awslocal ec2 authorize-security-group-ingress \
  --group-id $SG_ID2 \
  --protocol tcp \
  --port 0-65535 \
  --source-group $SG_ID1
</code></pre><p>Creates security groups for the load balancer and the ECS service, allowing necessary traffic.</p><pre><code class=language-bash>export LB_ARN=$(awslocal elbv2 create-load-balancer \
  --name ecs-load-balancer \
  --subnets $SUBNET_ID1 $SUBNET_ID2 \
  --security-groups $SG_ID1 \
  --scheme internet-facing \
  --type application \
  | jq -r '.LoadBalancers[0].LoadBalancerArn')
  
export TG_ARN=$(awslocal elbv2 create-target-group \
  --name ecs-targets \
  --protocol HTTP \
  --port 11434 \
  --vpc-id $VPC_ID \
  --target-type ip \
  --health-check-protocol HTTP \
  --region us-east-1 \
  --health-check-path / \
  | jq -r '.TargetGroups[0].TargetGroupArn')

awslocal elbv2 create-listener \
  --load-balancer-arn $LB_ARN \
  --protocol HTTP \
  --port 11434 \
  --default-actions Type=forward,TargetGroupArn=$TG_ARN
</code></pre><p>Creates an internet-facing application load balancer and a target group, and sets up a listener to forward traffic.</p><pre><code class=language-bash>awslocal ecr create-repository --repository-name ollama-service
export MODEL_NAME=tinyllama
docker build --build-arg MODEL_NAME=$MODEL_NAME -t ollama-service .
docker tag ollama-service:latest 000000000000.dkr.ecr.us-east-1.localhost.localstack.cloud:4510/ollama-service:latest
docker push 000000000000.dkr.ecr.us-east-1.localhost.localstack.cloud:4510/ollama-service:latest
</code></pre><p>Creates an ECR repository, builds the Docker image, and pushes it to the repository.</p><pre><code class=language-bash>awslocal ecs create-cluster --cluster-name OllamaCluster

awslocal iam create-role \
  --role-name ecsTaskRole \
  --assume-role-policy-document file://ecs-task-trust-policy.json
  
export ECS_TASK_PARN=$(awslocal iam create-policy \
  --policy-name ecsTaskPolicy \
  --policy-document file://ecs-task-policy.json \
  | jq -r '.Policy.Arn')
  
awslocal iam attach-role-policy \
  --role-name ecsTaskRole \
  --policy-arn $ECS_TASK_PARN
  
awslocal iam update-assume-role-policy \
  --role-name ecsTaskRole \
  --policy-document file://ecs-cloudwatch-policy.json

awslocal iam create-role \
  --role-name ecsTaskExecutionRole \
  --assume-role-policy-document file://ecs-trust-policy.json
  
export ECS_TASK_EXEC_PARN=$(awslocal iam create-policy \
  --policy-name ecsTaskExecutionPolicy \
  --policy-document file://ecs-task-exec-policy.json | jq -r '.Policy.Arn')
  
awslocal iam attach-role-policy \
  --role-name ecsTaskExecutionRole \
  --policy-arn $ECS_TASK_EXEC_PARN
  
awslocal iam update-assume-role-policy \
  --role-name ecsTaskExecutionRole \
  --policy-document file://ecs-cloudwatch-policy.json
</code></pre><p>Creates an ECS cluster and IAM roles with necessary policies for task execution.</p><pre><code class=language-bash>awslocal logs create-log-group --log-group-name ollama-service-logs
awslocal ecs register-task-definition \
  --family ollama-task \
  --cli-input-json file://task_definition.json
</code></pre><p>Creates a CloudWatch log group and registers the ECS task definition.</p><pre><code class=language-bash>awslocal ecs create-service \
  --cluster OllamaCluster \
  --service-name OllamaService \
  --task-definition ollama-task \
  --desired-count 2 \
  --launch-type FARGATE \
  --network-configuration &quot;awsvpcConfiguration={subnets=[$SUBNET_ID1,$SUBNET_ID2],securityGroups=[$SG_ID2],assignPublicIp=ENABLED}&quot; \
  --load-balancers &quot;targetGroupArn=$TG_ARN,containerName=ollama-container,containerPort=11434&quot;
</code></pre><p>Creates an ECS service with the specified configuration, linking it to the load balancer.</p><pre><code class=language-bash>awslocal s3 mb s3://frontend-bucket
awslocal s3 website s3://frontend-bucket --index-document index.html
awslocal s3api put-bucket-policy --bucket frontend-bucket --policy file://bucket-policy.json
awslocal s3 sync ./frontend/chatbot/build s3://frontend-bucket
</code></pre><p>Creates an S3 bucket, configures it as a website, sets the bucket policy, and syncs the frontend build to the bucket.</p><p>If you decide to use the AWS console to create all your resources, some of the complexity of these commands will be abstracted, and
some services will be created as dependencies of other resources.</p><p>You can run the full <code>commands.sh</code> script and watch the LocalStack logs for updated information on the resources, as they get created.
If you choose to, you can also manually run these commands, one by one, as you go through this article.</p><pre><code class=language-bash>bash commands.sh
</code></pre><h3 id=using-the-app-locally>Using the app locally</h3><p>Now that everything is deployed, you can go to the frontend application and try it out. In your browser, navigate to
<a href=http://frontend-bucket.s3-website.us-east-1.localhost.localstack.cloud:4566/><strong><code>http://frontend-bucket.s3-website.us-east-1.localhost.localstack.cloud:4566/</code></strong></a> and start typing your question. It takes a few seconds,
and then the full answer appears:</p><img class="img-simple img-fluid lazyload blur-up" src=/2024-07-04-develop-cloud-ai-with-ollama-localstack/ollama-localstack_hu86714ff2d71cd902f2a1720b073b72af_534642_20x0_resize_box_2.png data-src=/2024-07-04-develop-cloud-ai-with-ollama-localstack/ollama-localstack.png width=2398 height=1906 alt="App locally"><p>If you look at the <code>App.js</code>, located in <code>frontend/chatbot/src</code>, you&rsquo;ll notice the POST call payload contains a field <code>stream: false</code>.
For simplicity purpose we&rsquo;re going to receive our answer from the LLM &ldquo;in bulk&rdquo;, rather than streamed. This take a few seconds to
generate, and then it is fully received.</p><img class="img-simple img-fluid lazyload blur-up" src=/2024-07-04-develop-cloud-ai-with-ollama-localstack/ollama-call_huc53afadaf732e1d4231c22d518257baa_108533_20x0_resize_box_2.png data-src=/2024-07-04-develop-cloud-ai-with-ollama-localstack/ollama-call.png width=2460 height=492 alt="App locally"><p>The backend call will be made to the <strong>load balancer</strong>, at <a href=http://ecs-load-balancer.elb.localhost.localstack.cloud:4566/api/generate/><code>http://ecs-load-balancer.elb.localhost.localstack.cloud:4566/api/generate/</code></a>, so we don&rsquo;t have to worry
about how we access the task containers.</p><h2 id=running-on-aws>Running on AWS</h2><p>To run this stack in the real AWS cloud, we need to make some small adjustments.
The ready to deploy resources are in the <code>aws</code> folder, and they are the same as the ones for LocalStack, except:</p><ul><li>The AWS account number needs to be provided, so wherever you find <code>&lt;your_account_number></code>, it should be replaced with the 12-digit key (<code>task_definition_aws.json</code>,
<code>commands-aws.sh</code>).</li><li>A new bucket name needs to be set, as it needs to be unique, so the <code>&lt;your_bucket_name></code> placeholder has to be replaced with a name of your choice (<code>commands-s3-aws.sh</code>, <code>bucket-policy.json</code>).</li><li>Since on AWS you don&rsquo;t know the final DNS name of the load balancer, and we need it for the frontend component, we&rsquo;ll build the app and upload the files to the S3 bucket only after creating the stack.
The <code>commands-aws.sh</code> script will export and write the load balancer DNS name into the <code>.env</code> file, where the React app can pick it up from.
This is generally easier using LocalStack because the DNS name of the load balancer is always as defined by the user.</li></ul><p>The steps to getting this project on AWS are:</p><ol><li>Make the aforementioned changes to your files.</li><li>In the <code>aws</code> root folder run <code>bash commands-aws.sh</code>.</li><li>Build the React app:</li></ol><pre><code class=language-bash>cd frontend/chatbot
npm install
npm run build
</code></pre><ol start=4><li>Create the S3 bucket and prepare it to host the frontend application, by running <code>bash commands-aws-s3.sh</code> in the <code>aws</code> folder.</li></ol><p>After the first step, you can test the backend by running the following command:</p><pre><code class=language-bash>export LB_NAME=$(aws elbv2 describe-load-balancers --load-balancer-arns $LB_ARN | jq -r '.LoadBalancers[0].DNSName')
curl $LB_NAME
</code></pre><p>If you get a message like the following, give it a few more seconds until the Fargate instances are up and running and you should see the
<code>Ollama is running</code> response.</p><pre><code class=language-bash>&lt;html&gt;
&lt;head&gt;&lt;title&gt;503 Service Temporarily Unavailable&lt;/title&gt;&lt;/head&gt;
&lt;body&gt;
&lt;center&gt;&lt;h1&gt;503 Service Temporarily Unavailable&lt;/h1&gt;&lt;/center&gt;
&lt;/body&gt;
&lt;/html&gt;
</code></pre><p>After building the GUI part and uploading it to the S3 bucket, you&rsquo;ll be able to access your chatbot at this address:</p><pre><code class=language-bash>http://&lt;bucket-name&gt;.s3-website.us-east-1.amazonaws.com/
</code></pre><img class="img-simple img-fluid lazyload blur-up" src=/2024-07-04-develop-cloud-ai-with-ollama-localstack/ollama-on-aws_hue15e51b46b85a15a889788977da95fe2_387735_20x0_resize_box_2.png data-src=/2024-07-04-develop-cloud-ai-with-ollama-localstack/ollama-on-aws.png width=1708 height=1788 alt="App locally"><h2 id=conclusion>Conclusion</h2><p>Developing and testing cloud AI-powered applications can be complex, particularly when ensuring they perform reliably in a
production-like environment without incurring high costs. This is where integrating Ollama and LocalStack provides a robust solution.
Ollama, which simplifies the process of downloading, installing, and interacting with various large language models (LLMs), paired with
LocalStack&rsquo;s ability to emulate AWS cloud services locally, allows developers to rigorously test AI functionalities in a controlled and
cost-effective manner. By leveraging LocalStack, developers can validate integrations and behaviors
of AI models managed with Ollama.
The combination of Ollama&rsquo;s straightforward LLM handling with
LocalStack&rsquo;s comprehensive AWS emulation offers a powerful toolkit for any developer looking to build reliable and scalable cloud AI applications.</p></article></div></div></div></section><section class="section bg-gradient-light"><div class=container><div class="row justify-content-center"><div class="d-flex flex-column gap-4 col-lg-12 text-white"><h2 class="h2 text-center">Stay in the loop</h2><p>We'd love to get in touch with you.
Please subscribe with your email to stay tuned for release notes and product updates.
We promise never to send an excessive amount of emails (we hate spam, too).</p><script>hbspt.forms.create({region:"eu1",portalId:"26596507",formId:"1cc99237-4ce0-4d65-abdf-f87ac2724717"})</script></div></div></div></section><footer class="footer pt-6 bg-gradient-dark"><div class="footer-main pb-5"><div class=container><div class="row justify-content-between"><div class="col-12 col-lg-4"><img style=width:243px src=/images/header-logo-new.svg alt="LocalStack Logo"></div><div class="col-12 col-lg-8 mt-4 mt-lg-0"><div class="row gap-4"><div class=col><ul class="list-unstyled gap-4 d-flex flex-column"><li><a class=footer-text href=https://localstack.cloud/contact>Contact</a></li><li><a class=footer-text href=https://localstack.cloud/careers>Careers</a></li><li><a class=footer-text href=https://blog.localstack.cloud>Blog</a></li><li><a class=footer-text href=https://docs.localstack.cloud>Docs</a></li></ul></div><div class=col><ul class="list-unstyled gap-4 d-flex flex-column"><li><a class=footer-text href=https://docs.localstack.cloud/tutorials/>Tutorials</a></li><li><a class=footer-text href=https://docs.localstack.cloud/applications/>Applications</a></li><li><a class=footer-text href=https://docs.localstack.cloud/academy/>LocalStack Academy</a></li></ul></div><div class=col><ul class="list-unstyled gap-4 d-flex flex-column"><li><a class=footer-text href=https://www.localstack.cloud/legal/tos>Terms and Conditions</a></li><li><a class=footer-text href=https://www.localstack.cloud/legal/privacy-policy>Privacy Policy</a></li><li><a class=footer-text href=https://assets-global.website-files.com/6539036f80ddc9e9a467134e/657b15a7f5ba23e41984d5dd_LocalStack%20DPA.pdf>Data Processing Addendum</a></li></ul></div><div class=col><ul class="list-unstyled gap-4 d-flex flex-column"><li class=footer-text>Follow us</li><li class="d-flex gap-3"><a class=footer-text href=https://twitter.com/localstack><span><svg width="26" height="26" viewBox="0 0 26 26" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M25 13.25c0 6.9036-5.5964 12.5-12.5 12.5C5.59644 25.75.0 20.1536.0 13.25.0 6.34644 5.59644.75 12.5.75 19.4036.75 25 6.34644 25 13.25zm-6.56-3.0468C19.0443 10.1315 19.6205 9.97084 20.1562 9.73331 19.7558 10.3325 19.2493 10.8586 18.6651 11.2797 18.6709 11.4076 18.6738 11.5366 18.6738 11.6661c0 3.9487-3.0055 8.5014-8.5012 8.5014C8.48534 20.1675 6.91443 19.6731 5.59285 18.825 5.82615 18.8528 6.06457 18.8669 6.30522 18.8669 7.70545 18.8669 8.9936 18.3894 10.0164 17.5879 8.70861 17.5636 7.6054 16.6997 7.22527 15.5129 7.40754 15.5477 7.59449 15.5662 7.78745 15.5662 8.05952 15.5662 8.32401 15.5299 8.57447 15.4612c-1.36658-.2743-2.39692-1.4817-2.39692-2.9294C6.17755 12.5193 6.17755 12.5066 6.17777 12.4939 6.58041 12.7183 7.04144 12.8526 7.53098 12.868 6.7297 12.3323 6.20183 11.4174 6.20183 10.3811c0-.547520000000001.14751-1.06113.40465-1.50232C8.08024 10.687 10.282 11.8764 12.7651 12.001 12.7139 11.7822 12.6874 11.5542 12.6874 11.32c0-1.64975 1.338-2.98781 2.9878-2.98781.8599.0 1.6364.36276 2.1815.94322C18.5369 9.14149 19.1767 8.89304 19.754 8.55056 19.5305 9.24867 19.057 9.83358 18.44 10.2032z" fill="#fff"/></svg></span></a><a class=footer-text href=https://www.linkedin.com/company/localstack-cloud><span><svg width="26" height="26" viewBox="0 0 26 26" fill="none" xmlns="http://www.w3.org/2000/svg"><path fill-rule="evenodd" clip-rule="evenodd" d="M13 26c7.1797.0 13-5.8203 13-13S20.1797.0 13 0 0 5.8203.0 13 5.8203 26 13 26zM9.35126 10.5024C9.97963 10.5024 10.489 9.993 10.489 9.36463S9.97963 8.22687 9.35126 8.22687 8.2135 8.73626 8.2135 9.36463s.50939 1.13777 1.13776 1.13777zm2.21194.8622v6.3122h1.9599V14.5553C13.5231 13.7316 13.6781 12.9339 14.6993 12.9339c1.0073.0 1.0198.941700000000001 1.0198 1.6734v3.0701H17.68V14.2157C17.68 12.5153 17.3139 11.2086 15.3265 11.2086c-.9542.0-1.5938.5236-1.8554 1.0192H13.4446V11.3646H11.5632zm-3.19382.0H10.3324v6.3122H8.36938V11.3646z" fill="#fff"/></svg></span></a></li></ul></div></div></div></div></div></div><div class=footer-bottom><div class="container mb-2"><hr><div class="row pt-4"><div class=col style=color:#fff;opacity:.5>Copyright &copy; LocalStack</div><div class="col text-end"><ul class=list-inline></ul></div></div></div></div></footer><script src=/js/bootstrap.min.51b4083a09e54138d5bbe9c42a5f5f71af6e60a417f276d5620cbde3b15eb1a07c9ea17a889bc58074adf9b19641039a77521d3131f039300d2fe29ffa532aa8.js integrity="sha512-UbQIOgnlQTjVu+nEKl9fca9uYKQX8nbVYgy947FesaB8nqF6iJvFgHSt+bGWQQOad1IdMTHwOTANL+Kf+lMqqA==" crossorigin=anonymous defer></script><script src=/js/highlight.min.8cdbefd08ec63ef25f4b2510ae50c13b3d9ed43e25a15ddb55f14c33130e68cf8be3131ccf2d087c49f5f21fe88c2f0e2715a21f936047d0d0b85dc48cb864b9.js integrity="sha512-jNvv0I7GPvJfSyUQrlDBOz2e1D4loV3bVfFMMxMOaM+L4xMczy0IfEn18h/ojC8OJxWiH5NgR9DQuF3EjLhkuQ==" crossorigin=anonymous defer></script><script src=/main.min.8933efb6bcdc37606a11f732d52c6f65d7fc1d082b89d56516f6a1bfb528ee7c40c3430bb80214cdd129ec7d69d4cadd74e693da54a9ba6f08c44771204d4468.js integrity="sha512-iTPvtrzcN2BqEfcy1SxvZdf8HQgridVlFvahv7Uo7nxAw0MLuAIUzdEp7H1p1MrddOaT2lSpum8IxEdxIE1EaA==" crossorigin=anonymous defer></script><script src=/index.min.28297aad85d5f5902201eb4ef84e7012a37d4a34824e2256df80cd91e547bf155df22029e5596951d6f2692dd1fee54c76721b407e5ac526bd33bedc636a87e5.js integrity="sha512-KCl6rYXV9ZAiAetO+E5wEqN9SjSCTiJW34DNkeVHvxVd8iAp5VlpUdbyaS3R/uVMdnIbQH5axSa9M77cY2qH5Q==" crossorigin=anonymous defer></script></body></html>